{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ㅁㅁㅁㅉ (팀장 서승민)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. data \n",
    "> 데이터 일체\n",
    "\n",
    "- 00 : raw data\n",
    "- 01 : raw data에 original_c, small_c, middle_c, big_c 작업 마친 상태\n",
    "- 11 : 외부 변수 생성에 필요한 데이터(날씨, 클릭률 등)\n",
    "- 12 : dummy 변수 생성에 필요한 데이터\n",
    "- 13 : 최적화 모델링 및 counterfactual analysis 관련 데이터\n",
    "- 20 : preprocess 과정을 거친 데이터\n",
    "- saved_models: 훈련된 모델(bin type)\n",
    "\n",
    "\n",
    "#### 2. eda \n",
    "> eda 파일/클릭률 크롤링/계절성\n",
    "\n",
    "- eda.py : eda 과정 일체\n",
    "\n",
    "- naver_shopping_crawling.ipynb : 네이버 쇼핑 트렌드의 클릭률 데이터 crawling\n",
    "\n",
    "- naver_clickr_crawl.R : 클릭률 crawled data 통합\n",
    "\n",
    "\n",
    "#### 3. engine \n",
    "> feature engineering/train/predict/residual analysis /기타 변수 정의\n",
    "\n",
    "- features.py : feature engineering 과정\n",
    "- predict.py : train.py를 통해 훈련된 모델 weight을 불러와서 2020년 6월 매출 예측\n",
    "- residual_analysis.ipynb : 훈련 결과 residual analysis\n",
    "- train.py : preprocess + 모델 훈련 + cross validation\n",
    "- utils.py : preprocess + helper + data split에 필요한 함수 모음\n",
    "- vars.py : 자주 사용하는 변수 모음\n",
    "\n",
    "#### 4. opt\n",
    "> 최적화 모델/counterfactual 관련 코드\n",
    "\n",
    "- counterfactual.py : counterfactual analysis 관련 코드\n",
    "- inputs.py : 헝가리안 최적화 알고리즘을 위한 input 생성\n",
    "- opt_model.py : 헝가리안 최적화 적용 코드\n",
    "\n",
    "#### 5. plot \n",
    "> eda.py 파일 실행 후 생성된 plot. PPT에 사용\n",
    "\n",
    "#### 6. submission \n",
    "- submission.xlsx : 2020년 6월 편성표 + predicted y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# model\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "TARGET = '취급액'\n",
    "\n",
    "# directories\n",
    "LOCAL_DIR = \"../\"\n",
    "MODELS_DIR = LOCAL_DIR + \"data/saved_models/\"\n",
    "PROCESSED_DATA_DIR = LOCAL_DIR + \"data/20/\"\n",
    "FEATURED_DATA_DIR = LOCAL_DIR + \"data/20/\"\n",
    "SUBMISSION_DIR = LOCAL_DIR + \"submission/\"\n",
    "OPT_DATA_DIR = LOCAL_DIR + \"data/13/\"\n",
    "RAW_DATA_DIR = LOCAL_DIR + \"data/00/\"\n",
    "\n",
    "# set global vars\n",
    "data_list = ['df_wk_lag', 'df_wk_no_lag', 'df_wd_lag', 'df_wd_no_lag', 'df_all_lag']\n",
    "\n",
    "lag_col1 = ['lag_scode_price', 'lag_scode_count', 'lag_mcode_price', 'lag_mcode_count', 'lag_bigcat_price',\n",
    "            'lag_bigcat_count', 'lag_bigcat_price_day', 'lag_bigcat_count_day', 'lag_small_c_price',\n",
    "            'lag_small_c_count', 'lag_all_price_show', 'lag_all_price_day']\n",
    "\n",
    "lag_col2 = ['ts_pred', 'rolling_mean_7', 'rolling_mean_14', 'rolling_mean_21',\n",
    "            'rolling_mean_28', 'mean_sales_origin']\n",
    "\n",
    "lag_wd = ['lag_sales_wd_1', 'lag_sales_wd_2', 'lag_sales_wd_3','lag_sales_wd_4', 'lag_sales_wd_5']\n",
    "lag_wk = ['lag_sales_wk_1', 'lag_sales_wk_2']\n",
    "full_lag_col = ['lag_sales_1', 'lag_sales_2', 'lag_sales_5', 'lag_sales_7']\n",
    "\n",
    "cat_col = ['상품군', 'weekdays', 'show_id', 'small_c', 'middle_c', 'big_c',\n",
    "           'pay', 'months', 'hours_inweek', 'weekends', 'japp', 'parttime',\n",
    "           'min_start', 'primetime', 'prime_smallc',\n",
    "           'freq', 'bpower', 'steady', 'men', 'pay', 'luxury',\n",
    "           'spring', 'summer', 'fall', 'winter', 'rain']\n",
    "\n",
    "encoded_cols = ['상품코드', '상품군', 'weekdays', 'parttime', 'show_id','small_c', 'middle_c',\n",
    "                'big_c', 'original_c', 'pay', 'exposed_t']\n",
    "\n",
    "base_cols = ['방송일시', '노출(분)', '마더코드', '상품코드', '상품명', '상품군', '판매단가', '취급액']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path):\n",
    "    \"\"\"\n",
    "    :objective: load data\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_pickle(path)\n",
    "        return df.reset_index()\n",
    "    except:\n",
    "        print(\"check file directory\")\n",
    "\n",
    "\n",
    "def drop_useless(df):\n",
    "    \"\"\"\n",
    "    :objective: drop useless features for model.\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    #useless features\n",
    "    xcol = ['방송일시', '노출(분)', '마더코드', '상품명', 'exposed', 'ymd', 'volume',\n",
    "            'years', 'days', 'hours', 'week_num', 'holidays', 'red', 'min_range', 'brand',\n",
    "            'small_c_code', 'middle_c_code', 'big_c_code', 'sales_power']\n",
    "    col = [x for x in df.columns if x in xcol]\n",
    "    df = df.drop(columns=col)\n",
    "    df = df.copy()\n",
    "    return df\n",
    "\n",
    "def check_na(df):\n",
    "    \"\"\"\n",
    "    :objective: show na\n",
    "    :return: columns with na / na counts\n",
    "    \"\"\"\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "def na_to_zeroes(df):\n",
    "    \"\"\"\n",
    "    :objective: Change all na's to zero.(just for original lag!)\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    xcol = [x for x in df.columns if\n",
    "            x in lag_col1 + lag_col2 + ['mid_click_r', 'age30_middle', 'age40_middle', 'age50_middle',\n",
    "                                        'age60above_middle', 'pc_middle', 'mobile_middle']]\n",
    "    for col in xcol:\n",
    "        df[col] = df[col].fillna(0)\n",
    "    return df\n",
    "\n",
    "def run_label_all(df):\n",
    "    \"\"\"\n",
    "    :objective: Perform labelencoding for all categorical/object columns\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    lab_col = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "    for col in lab_col:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].values)\n",
    "    return df\n",
    "\n",
    "def run_preprocess(df):\n",
    "    \"\"\"\n",
    "    :objective: Run Feature deletion, NA imputation, label encoding\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    df = drop_useless(df)\n",
    "    df = na_to_zeroes(df)\n",
    "    df = run_label_all(df)\n",
    "    df1 = df.copy()\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeder\n",
    "def seed_everything(seed=127):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# metrics\n",
    "# negative mape (For Bayesian Optimization)\n",
    "def neg_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    result = (-1) * mape\n",
    "    return result\n",
    "\n",
    "# MAPE\n",
    "def get_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    final = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return final\n",
    "\n",
    "# RMSE\n",
    "def get_rmse(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    return rmse\n",
    "\n",
    "# MAE\n",
    "def get_mae(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    return mae\n",
    "\n",
    "\n",
    "def cv_split(df, month, printprop=False):\n",
    "    \"\"\"\n",
    "    :objective: get index to create cross validation dataset\n",
    "    :param df: pandas dataframe\n",
    "    :param month: int - from 1 to 12, month to be splited\n",
    "    :param printprop: boolean - whether to print proportion of cv to full data\n",
    "    :return: int - index for full data to be splited\n",
    "    \"\"\"\n",
    "    split = int(df[df['months'] == month].index.values.max())\n",
    "    prop = str(split / df.shape[0])\n",
    "    if printprop:\n",
    "        print(f'Proportion of train set is {prop}')\n",
    "        return split\n",
    "    else:\n",
    "        return split\n",
    "\n",
    "\n",
    "def divide_train_val(df_pp, month, drop):\n",
    "    \"\"\"\n",
    "    :objective: divide full data into train, validation\n",
    "    :param df_pp: pandas dataframe, preprocessed\n",
    "    :param month: int - from 1 to 12, month to be splited\n",
    "    :param drop: list of str - columns to be dropped\n",
    "    :return: pd.DataFrame\n",
    "    \"\"\"\n",
    "    split = cv_split(df=df_pp, month=month)\n",
    "    train_x = df_pp.iloc[:split, :].drop(columns=['index',\n",
    "                                                  'show_id', TARGET] + drop)  ## 'index' check!!\n",
    "    train_y = df_pp.iloc[:split, :][TARGET]\n",
    "    val_x = df_pp.iloc[split:, :].drop(columns=['index',\n",
    "                                                'show_id', TARGET] + drop)\n",
    "    val_y = df_pp.iloc[split:, :][TARGET]\n",
    "    return train_x, train_y, val_x, val_y\n",
    "\n",
    "\n",
    "def divide_top(df, num_train, num_val):\n",
    "    \"\"\"\n",
    "    :objective: divide full data by mean_sales_origin ranking\n",
    "    :param df: pandas dataframe\n",
    "    :param num_train: int - index to divide train and val\n",
    "    :param num_val: int - index to divide train and val\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    top_df = df.sort_values('mean_sales_origin', ascending=False)\n",
    "\n",
    "    top_tr_lag_x = top_df.iloc[:num_train, :].drop(['index', 'show_id', TARGET], axis=1)\n",
    "    top_tr_lag_y = top_df.iloc[:num_train, :][TARGET]\n",
    "    top_v_lag_x = top_df.iloc[num_train:(num_train + num_val), :].drop(['index', 'show_id', TARGET], axis=1)\n",
    "    top_v_lag_y = top_df.iloc[num_train:(num_train + num_val), :][TARGET]\n",
    "\n",
    "    return top_df, top_tr_lag_x, top_tr_lag_y, top_v_lag_x, top_v_lag_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features.py 를 실행하게 되면 아래와 같은 과정을 거쳐 <br />\n",
    "\n",
    "Features(types=\"train\", not_divided=False) 클래스를 임포트할 수 있고, <br />\n",
    "**:params types:** str - \"train\",\"test\",\"hungarian\",\"hungarian_h1\"<br />\n",
    "**:params not_divided:** boolean - wk,wd 나눠진 데이터인지, 통합데이터(구분X)인지<br />\n",
    "**:params counterfactual:** boolean - counterfactual input 생성에 사용 <br />\n",
    "\n",
    "train, test 데이터를 불러와 feature를 생성하고,<br />\n",
    "직접 클래스를 import 하는 방식으로 헝가리안 알고리즘, counterfactual analysis에 필요한 인풋을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lag = Features(types=\"train\", not_divided=False)\n",
    "train_lag.run_all()\n",
    "train_full = Features(types=\"train\", not_divided=True)\n",
    "train_full.run_all()\n",
    "test_lag = Features(types=\"test\", not_divided=False)\n",
    "test_lag.run_all()\n",
    "test_lag = Features(types=\"test\", not_divided=True)\n",
    "test_lag.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_fin_wd_lag.pkl\n",
    "- train_fin_wk_lag.pkl \n",
    "- train_light_ver.pkl \n",
    "- train_v2.pkl\n",
    "- test_fin_wd_lag.pkl\n",
    "- test_fin_wk_lag.pkl \n",
    "- test_light_ver.pkl \n",
    "\n",
    "총 7개의 featured data가 생성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import 4 types of dataset\n",
    "Descriptions:\n",
    "- df_wd_lag : weekday / + lags\n",
    "- df_wk_lag: weekend / + lags\n",
    "- df_wd_test : weekday / + lags on test data\n",
    "- df_wk_test: weekend / + lags on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wd_lag = load_df(FEATURED_DATA_DIR + '/train_fin_wd_lag.pkl')\n",
    "df_wk_lag = load_df(FEATURED_DATA_DIR + '/train_fin_wk_lag.pkl')\n",
    "\n",
    "df_wd_test = load_df(FEATURED_DATA_DIR + '/test_fin_wd_lag.pkl')\n",
    "df_wk_test = load_df(FEATURED_DATA_DIR + '/test_fin_wk_lag.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding & Divide train/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined data for label encoding\n",
    "tmp_combined = pd.concat([df_wd_lag, df_wk_lag, df_wd_test, df_wk_test]).drop(columns=['index'])\n",
    "\n",
    "# Preprocessed datasets\n",
    "tmp_combined = run_preprocess(tmp_combined)\n",
    "df_wd_lag_PP = tmp_combined.loc[:, tmp_combined.columns.isin(df_wd_lag.columns)].iloc[:df_wd_lag.shape[0]].reset_index()\n",
    "df_wk_lag_PP = tmp_combined.loc[:, tmp_combined.columns.isin(df_wk_lag.columns)]\\\n",
    "                .iloc[df_wd_lag.shape[0]:(df_wd_lag.shape[0]+df_wk_lag.shape[0])].reset_index()\n",
    "df_wd_test_PP = tmp_combined.loc[:, tmp_combined.columns.isin(df_wd_test.columns)]\\\n",
    "                .iloc[(df_wd_lag.shape[0]+df_wk_lag.shape[0]):(df_wd_lag.shape[0]+df_wk_lag.shape[0]+df_wd_test.shape[0])]\n",
    "df_wk_test_PP = tmp_combined.loc[:, tmp_combined.columns.isin(df_wk_test.columns)].iloc[-df_wk_test.shape[0]:]\n",
    "\n",
    "# write pickle for test data\n",
    "df_wd_test_PP.to_pickle(FEATURED_DATA_DIR + 'test_fin_wd_PP.pkl')\n",
    "df_wk_test_PP.to_pickle(FEATURED_DATA_DIR + 'test_fin_wk_PP.pkl')\n",
    "# Divide data\n",
    "# WD\n",
    "train_wd_lag_x, train_wd_lag_y, val_wd_lag_x, val_wd_lag_y = divide_train_val(df_wd_lag_PP, 8, drop=[])\n",
    "top_wd_lag, top_tr_wd_lag_x, top_tr_wd_lag_y, top_v_wd_lag_x, top_v_wd_lag_y = divide_top(df_wd_lag_PP, 4004, 2013)\n",
    "# WK\n",
    "train_wk_lag_x, train_wk_lag_y, val_wk_lag_x, val_wk_lag_y = divide_train_val(df_wk_lag_PP, 8, drop=[])\n",
    "top_wk_lag, top_tr_wk_lag_x, top_tr_wk_lag_y, top_v_wk_lag_x, top_v_wk_lag_y = divide_top(df_wk_lag_PP, 2206, 999)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Two-stage LightGBM (Top & All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model structure\n",
    "def run_lgbm(params, train_x, train_y, val_x, val_y, df_type='wd_all'):\n",
    "    \"\"\"\n",
    "    :objective: run lgbm model\n",
    "    :param params: dictionary\n",
    "    :param train_x: pd.DataFrame\n",
    "    :param train_y: pd.DataFrame\n",
    "    :param val_x: pd.DataFrame\n",
    "    :param val_y: pd.DataFrame\n",
    "    :param df_type: str - 'wd_all', 'wk_all', 'wd_top', 'wk_top'\n",
    "    :return: LGBMRegressor, np.array\n",
    "    \"\"\"\n",
    "\n",
    "    seed_everything(seed=127)\n",
    "\n",
    "    model_lg = LGBMRegressor(**params)\n",
    "    model_lg.fit(train_x, train_y)\n",
    "    lgbm_preds = model_lg.predict(val_x)\n",
    "\n",
    "    # Plot LGBM: Predicted vs. True values\n",
    "    plt.figure(figsize= (40,5))\n",
    "    plt.rcParams[\"axes.grid.axis\"] = \"y\"\n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "    x = range(0, len(lgbm_preds))\n",
    "    plt.plot(x, val_y, label='true', marker='', color='grey', linewidth=2, alpha=0.8)\n",
    "    plt.plot(x, lgbm_preds, label='predicted', marker='', color='tomato', linewidth=2)\n",
    "    pop_b = mpatches.Patch(color='tomato', label='Predicted')\n",
    "    pop_c = mpatches.Patch(color='grey', label='True')\n",
    "    plt.legend(handles=[pop_b, pop_c], fontsize=27, loc=2)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=25)\n",
    "    plt.ylabel('Sales', fontsize=25)\n",
    "    plt.show()\n",
    "\n",
    "    # Get Scores\n",
    "    print(f'MAPE of best iter is {get_mape(val_y, lgbm_preds)}')\n",
    "    print(f'MAE of best iter is {get_mae(val_y, lgbm_preds)}')\n",
    "\n",
    "    model_name = MODELS_DIR + 'lgbm_finalmodel_' + df_type + '.bin'\n",
    "    pickle.dump(model_lg, open(model_name, 'wb'))\n",
    "\n",
    "    return model_lg, lgbm_preds\n",
    "\n",
    "##################################################################\n",
    "################# Step 1. For ALL observations ###################\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# parameters for wd/wk model\n",
    "params_all_wd = {'feature_fraction': 1,\n",
    "                 'learning_rate': 0.001,\n",
    "                 'min_data_in_leaf': 135,\n",
    "                 'n_estimators': 3527,\n",
    "                 'num_iterations': 2940,\n",
    "                 'subsample': 1,\n",
    "                 'boosting_type': 'dart',\n",
    "                 'objective': 'regression',\n",
    "                 'metric': 'mape',\n",
    "                 'categorical_feature': [3, 9, 10, 11]  ## weekdays, small_c, middle_c, big_c\n",
    "                 }\n",
    "\n",
    "params_all_wk = {'feature_fraction': 1,\n",
    "                 'learning_rate': 0.001,\n",
    "                 'min_data_in_leaf': 134,\n",
    "                 'n_estimators': 3474,\n",
    "                 'num_iterations': 2928,\n",
    "                 'subsample': 1,\n",
    "                 'boosting_type': 'dart',\n",
    "                 'objective': 'regression',\n",
    "                 'metric': 'mape',\n",
    "                 'categorical_feature': [3, 9, 10, 11]}  ## weekdays, small_c, middle_c, big_c\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "################### Step 2. For High-rank observations ###################\n",
    "###########################################################################\n",
    "\n",
    "params_top_wd = {'feature_fraction': 1,\n",
    "                 'learning_rate': 0.0025,\n",
    "                 'min_data_in_leaf': 70,\n",
    "                 'n_estimators': 5000,\n",
    "                 'num_iterations': 4000,\n",
    "                 'subsample': 1,\n",
    "                 'boosting_type': 'dart',\n",
    "                 'objective': 'regression',\n",
    "                 'metric': 'mape',\n",
    "                 'categorical_feature': [3, 9, 10, 11]  ## weekdays, small_c, middle_c, big_c\n",
    "                 }\n",
    "\n",
    "params_top_wk = {'feature_fraction': 1,\n",
    "                 'learning_rate': 0.0025,\n",
    "                 'min_data_in_leaf': 30,\n",
    "                 'n_estimators': 5000,\n",
    "                 'num_iterations': 3500,\n",
    "                 'subsample': 1,\n",
    "                 'boosting_type': 'dart',\n",
    "                 'objective': 'regression',\n",
    "                 'metric': 'mape',\n",
    "                 'categorical_feature': [3, 9, 10, 11]  ## weekdays, small_c, middle_c, big_c\n",
    "                 }\n",
    "\n",
    "#####################################################################\n",
    "############## Step 3. Mix results from step1 & step2 ###############\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "def mixed_df(model_top, top_df, val_all_df_x, preds_all, num_top):\n",
    "    \"\"\"\n",
    "    :objective: combine results from step 1&2\n",
    "    :param model_top: LGBMRegressor - step 2 model\n",
    "    :param top_df: pandas Dataframe - divided df with high mean_sales_origin(by divide_top_function)\n",
    "    :param val_all_df_x: pandas Dataframe - validation x \n",
    "    :param preds_all: predicted y from step 1\n",
    "    :param num_top: int - index to be splitted\n",
    "    :return: pandas DataFrame\n",
    "    \"\"\"\n",
    "    top_idx = set(top_df.iloc[:num_top, :].index)\n",
    "    val_idx = set(val_all_df_x.index)\n",
    "    top_in_val = list(val_idx.intersection(top_idx))\n",
    "\n",
    "    val_copy = val_all_df_x.copy()\n",
    "    val_copy[TARGET] = preds_all\n",
    "\n",
    "    for i in top_in_val:\n",
    "        val_copy[TARGET].loc[val_copy.index == i] = model_top.predict(val_all_df_x.loc[val_all_df_x.index == i])\n",
    "\n",
    "    return val_copy\n",
    "\n",
    "\n",
    "def mix_results(true_y, pred_y):\n",
    "    \"\"\"\n",
    "    :objective: draw plot of true and estimated value\n",
    "    :param true_y: pandas Series\n",
    "    :param pred_y: pandas Series\n",
    "    :return: plot figure\n",
    "    \"\"\"\n",
    "    # Plot TOP: Predicted vs. True values\n",
    "    plt.figure(figsize=(40, 5))\n",
    "    plt.rcParams[\"axes.grid.axis\"] = \"y\"\n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "    x = range(0, len(true_y))\n",
    "    plt.plot(x, true_y, label='true', marker='', color='grey', linewidth=2, alpha=0.8)\n",
    "    plt.plot(x, pred_y, label='predicted', marker='', color='tomato', linewidth=2)\n",
    "    pop_b = mpatches.Patch(color='tomato', label='Predicted')\n",
    "    pop_c = mpatches.Patch(color='grey', label='True')\n",
    "    plt.legend(handles=[pop_b, pop_c], fontsize=27, loc=2)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.ylabel('Sales', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'MAPE of mixed model is {get_mape(true_y, pred_y)}')\n",
    "    print(f'MAE of mixed model is {get_mae(true_y, pred_y)}')\n",
    "    print(f'RMSE of mixed model is {get_rmse(true_y, pred_y)}')\n",
    "\n",
    "\n",
    "def run_models():\n",
    "    \"\"\"\n",
    "    :objective: train model!\n",
    "    \"\"\"\n",
    "    # base model\n",
    "    model_wd_all, preds_wd_all = run_lgbm(params_all_wd, train_wd_lag_x, train_wd_lag_y,\n",
    "                                          val_wd_lag_x, val_wd_lag_y, 'wd_all')\n",
    "    model_wk_all, preds_wk_all = run_lgbm(params_all_wk, train_wk_lag_x, train_wk_lag_y,\n",
    "                                          val_wk_lag_x, val_wk_lag_y, 'wk_all')\n",
    "    # top model\n",
    "    model_wd_top, preds_wd_top = run_lgbm(params_top_wd, top_tr_wd_lag_x, top_tr_wd_lag_y,\n",
    "                                          top_v_wd_lag_x, top_v_wd_lag_y, 'wd_top')\n",
    "    model_wk_top, preds_wk_top = run_lgbm(params_top_wk, top_tr_wk_lag_x, top_tr_wk_lag_y,\n",
    "                                          top_v_wk_lag_x, top_v_wk_lag_y, 'wk_top')\n",
    "    # mixed\n",
    "    mixed_wd = mixed_df(model_wd_top, top_wd_lag, val_wd_lag_x, preds_wd_all, num_top=6017)\n",
    "    mix_results(val_wd_lag_y, mixed_wd[TARGET])\n",
    "    mixed_wk = mixed_df(model_wk_top, top_wk_lag, val_wk_lag_x, preds_wk_all, num_top=3205)\n",
    "    mix_results(val_wk_lag_y, mixed_wk[TARGET])\n",
    "\n",
    "    return mixed_wd, mixed_wk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-1. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for Single LightGBM & Two-stage LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(cv_months=[7, 8, 9]):\n",
    "\n",
    "    # for step 1 model\n",
    "    # wd\n",
    "    for num in cv_months:\n",
    "        month = num\n",
    "        train_x_wd, train_y_wd, val_x_wd, val_y_wd = divide_train_val(df_wd_lag_PP, month, drop=[])\n",
    "        print(f'WD - CV with month {month} is starting.')\n",
    "        run_lgbm(params_all_wd, train_x_wd, train_y_wd, val_x_wd, val_y_wd, 'wd_all')\n",
    "\n",
    "    # wk\n",
    "    for num in cv_months:\n",
    "        month = num\n",
    "        train_x_wk, train_y_wk, val_x_wk, val_y_wk = divide_train_val(df_wk_lag_PP, month, drop=[])\n",
    "        print(f'WK - CV with month {month} is starting.')\n",
    "        run_lgbm(params_all_wk, train_x_wk, train_y_wk, val_x_wk, val_y_wk, 'wk_all')\n",
    "\n",
    "    # for step 3 model\n",
    "    # wd\n",
    "    cv_wd = [[2952, 1052, 12], [4524, 2093, 40]]\n",
    "    for num in cv_months:\n",
    "        print(f'WD - CV for Mixed model - month {num} is starting.')\n",
    "        for lst in cv_wd:\n",
    "            print(f'WD - CV for Mixed model - top {lst[2]}% is starting.')\n",
    "            train = lst[0]\n",
    "            val = lst[1]\n",
    "            train_x_wd_all, train_y_wd_all, val_x_wd_all, val_y_wd_all = divide_train_val(df_wd_lag_PP, num, drop=[])\n",
    "            top_cv, train_x_wd_top, train_y_wd_top, val_x_wd_top, val_y_wd_top = divide_top(df_wd_lag_PP, train, val)\n",
    "            model_all_cv, preds_all_cv = run_lgbm(params_all_wd, train_x_wd_all, train_y_wd_all, val_x_wd_all, val_y_wd_all,\n",
    "                                                  'wd_all')\n",
    "            model_top_cv, preds_top_cv = run_lgbm(params_top_wd, train_x_wd_top, train_y_wd_top, val_x_wd_top, val_y_wd_top,\n",
    "                                                  'wd_top')\n",
    "            mixed_wd = mixed_df(model_top_cv, top_cv, val_x_wd_all, preds_all_cv, num_top=(lst[0] + lst[1]))\n",
    "            mix_results(val_y_wd_all, mixed_wd[TARGET])\n",
    "\n",
    "    # wk\n",
    "    cv_wk = [[1205, 504, 16], [2856, 1359, 40]]\n",
    "    for num in cv_months:\n",
    "        print(f'WK - CV for Mixed model - month {num} is starting.')\n",
    "        for lst in cv_wk:\n",
    "            print(f'WK - CV for Mixed model - top {lst[2]}% is starting.')\n",
    "            train = lst[0]\n",
    "            val = lst[1]\n",
    "            train_x_wk_all, train_y_wk_all, val_x_wk_all, val_y_wk_all = divide_train_val(df_wk_lag_PP, num, drop=[])\n",
    "            top_cv, train_x_wk_top, train_y_wk_top, val_x_wk_top, val_y_wk_top = divide_top(df_wk_lag_PP, train, val)\n",
    "            _, preds_all_cv = run_lgbm(params_all_wk, train_x_wk_all, train_y_wk_all, val_x_wk_all, val_y_wk_all, 'wk_all')\n",
    "            model_top_cv, _ = run_lgbm(params_top_wk, train_x_wk_top, train_y_wk_top, val_x_wk_top, val_y_wk_top, 'wk_top')\n",
    "            mixed_wk = mixed_df(model_top_cv, top_cv, val_x_wk_all, preds_all_cv, num_top=(lst[0] + lst[1]))\n",
    "            mix_results(val_y_wk_all, mixed_wk[TARGET])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-2. Robust Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict December by January - August Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19년 1-8월의 데이터로 훈련된 모델을 사용해 12월을 예측해봄으로써 <br/> \n",
    "time gap이 존재하는 상황에서 우리 모델의 퍼포먼스를 점검해보고 좀더 time-robust한 모델을 build 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_cross_validation():\n",
    "    \"\"\"\n",
    "    :objective: perform robust cv; predicting 2019 Dec with our model, trained by 2019 Jan to 2019 Aug sales data\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    # Data preparation, Jan to Aug\n",
    "    train_x_wd_rb, train_y_wd_rb, val_x_wd_rb, val_y_wd_rb = divide_train_val(df_wd_lag_PP, 8, drop=[])\n",
    "    top_wd_rb, top_tr_x_wd_rb, top_tr_y_wd_rb, top_v_x_wd_rb, top_v_y_wd_rb = divide_top(df_wd_lag_PP, 4004, 2013)\n",
    "    train_x_wk_rb, train_y_wk_rb, val_x_wk_rb, val_y_wk_rb = divide_train_val(df_wk_lag_PP, 8, drop=[])\n",
    "    top_wk_rb, top_tr_x_wk_rb, top_tr_y_wk_rb, top_v_x_wk_rb, top_v_y_wk_rb = divide_top(df_wk_lag_PP, 2206, 999)\n",
    "    # target - 2019 Dec\n",
    "    DEC = 12\n",
    "    wd_dec_x = df_wd_lag_PP[df_wd_lag_PP.months == DEC].drop(['index', 'show_id', TARGET], axis=1)\n",
    "    wd_dec_y = df_wd_lag_PP[df_wd_lag_PP.months == DEC][TARGET]\n",
    "    wk_dec_x = df_wk_lag_PP[df_wk_lag_PP.months == DEC].drop(['index', 'show_id', TARGET], axis=1)\n",
    "    wk_dec_y = df_wk_lag_PP[df_wk_lag_PP.months == DEC][TARGET]\n",
    "\n",
    "    # wd\n",
    "    model_all_rb, preds_all_rb = run_lgbm(params_all_wd, train_x_wd_rb, train_y_wd_rb,\n",
    "                                          val_x_wd_rb, val_y_wd_rb, 'wd_all')\n",
    "    model_top_rb, _ = run_lgbm(params_top_wd, top_tr_x_wd_rb, top_tr_y_wd_rb, top_v_x_wd_rb, top_v_y_wd_rb,'wd_top')\n",
    "    preds_wd_dec = model_all_rb.predict(wd_dec_x)\n",
    "    mixed_wd_rb = mixed_df(model_top_rb, top_wd_rb, wd_dec_x, preds_wd_dec, num_top=6017)\n",
    "    mix_results(wd_dec_y, mixed_wd_rb[TARGET])\n",
    "\n",
    "    # wk\n",
    "    model_all_rb, _ = run_lgbm(params_all_wk, train_x_wk_rb, train_y_wk_rb, val_x_wk_rb, val_y_wk_rb, 'wk_all')\n",
    "    model_top_rb, _ = run_lgbm(params_top_wk, top_tr_x_wk_rb, top_tr_y_wk_rb, top_v_x_wk_rb, top_v_y_wk_rb,'wk_top')\n",
    "    preds_wk_dec = model_all_rb.predict(wk_dec_x)\n",
    "    mixed_wk_rb = mixed_df(model_top_rb, top_wk_rb, wk_dec_x, preds_wk_dec, num_top=3205)\n",
    "    mix_results(wk_dec_y, mixed_wk_rb[TARGET])\n",
    "\n",
    "    return mixed_wd_rb, mixed_wk_rb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "run_models()\n",
    "cross_validation(cv_months=[7, 8, 9])\n",
    "robust_cross_validation()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Test data by our model (Two-stage LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    \"\"\"\n",
    "    :objective: run model on test data\n",
    "    :return: pd.DataFrame, pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Load Models\n",
    "    model_path = MODELS_DIR + 'lgbm_finalmodel_wd_all.bin'\n",
    "    model_wd_all = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "    model_path = MODELS_DIR + 'lgbm_finalmodel_wd_top.bin'\n",
    "    model_wd_top = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "    model_path = MODELS_DIR + 'lgbm_finalmodel_wk_all.bin'\n",
    "    model_wk_all = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "    model_path = MODELS_DIR + 'lgbm_finalmodel_wk_top.bin'\n",
    "    model_wk_top = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "    # wd\n",
    "    test_wd_origin = load_df(FEATURED_DATA_DIR + 'test_fin_wd_lag.pkl')\n",
    "    test_wd = load_df(FEATURED_DATA_DIR + 'test_fin_wd_PP.pkl').copy()\n",
    "    test_wd = test_wd.drop(['index', 'show_id', TARGET], axis=1)\n",
    "    test_wd_sort = test_wd.sort_values('mean_sales_origin', ascending=False)\n",
    "    # Predict all observations\n",
    "    pred_test_wd_all = model_wd_all.predict(test_wd)\n",
    "    # Mixed DF (Top: 727개)\n",
    "    test_mixed_wd = mixed_df(model_wd_top, test_wd_sort, test_wd, pred_test_wd_all, num_top=727)\n",
    "    test_wd_origin[TARGET] = test_mixed_wd[TARGET]\n",
    "\n",
    "    # wk\n",
    "    test_wk_origin = load_df(FEATURED_DATA_DIR + 'test_fin_wk_lag.pkl')\n",
    "    test_wk = load_df(FEATURED_DATA_DIR + 'test_fin_wk_PP.pkl').copy()\n",
    "    test_wk = test_wk.drop(['index', 'show_id', TARGET], axis=1)\n",
    "    test_wk_sort = test_wk.sort_values('mean_sales_origin', ascending=False)\n",
    "    # Predict all observations\n",
    "    pred_test_wk_all = model_wk_all.predict(test_wk)\n",
    "    # Mixed DF (Top: 249개)\n",
    "    test_mixed_wk = mixed_df(model_wk_top, test_wk_sort, test_wk, pred_test_wk_all, num_top=249)\n",
    "    test_wk_origin[TARGET] = test_mixed_wk[TARGET]\n",
    "    # two outputs\n",
    "    return test_wd_origin.drop(columns = ['index']), test_wk_origin.drop(columns = ['index'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the submission file (including observations with 0 sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(wd, wk):\n",
    "    \"\"\"\n",
    "    create submission file\n",
    "    :param wd: pd.DataFrame\n",
    "    :param wk: pd.DataFrame\n",
    "    :return: pd.DataFrame\n",
    "    \"\"\"\n",
    "    test_origin = pd.read_excel(RAW_DATA_DIR + \"202006schedule.xlsx\", skiprows=1)\n",
    "    test_origin = test_origin.loc[(test_origin.판매단가 == 0)]\n",
    "    test_origin = test_origin[['방송일시', '노출(분)', '마더코드', '상품코드', '상품명', '상품군', '판매단가', TARGET]]\n",
    "    test_origin[TARGET] = 0\n",
    "    test_final_wd = wd[['방송일시', '노출(분)', '마더코드', '상품코드', '상품명', '상품군', '판매단가', TARGET]]\n",
    "    test_final_wk = wk[['방송일시', '노출(분)', '마더코드', '상품코드', '상품명', '상품군', '판매단가', TARGET]]\n",
    "    test_final_full = pd.concat([test_origin, test_final_wd, test_final_wk], axis=0)\n",
    "    test_final_full.sort_values(['방송일시'], inplace=True)\n",
    "\n",
    "    test_final_full.to_excel(SUBMISSION_DIR + 'submission.xlsx', index=False)\n",
    "    return test_final_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test_wd_origin, test_wk_origin = predict()\n",
    "submission(test_wd_origin, test_wk_origin)\n",
    "print(\"finish to create submission files\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
