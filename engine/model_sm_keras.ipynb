{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import sys, gc, time\n",
    "import os\n",
    "\n",
    "# data\n",
    "import datetime\n",
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# visualize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler #StandardScaler\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# model\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# custom modules\n",
    "# from engine.features_yj import Features\n",
    "from preprocess import load_df_added, drop_useless, check_na, run_label_all, remove_outliers, run_stdscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_DIR = os.getcwd()\n",
    "featured_DATA_DIR = '../data/20'\n",
    "# PROCESSED_DATA_DIR = local_DIR +'/data/21'\n",
    "\n",
    "df_wd_lag = pd.read_pickle(featured_DATA_DIR + '/train_fin_wd_lag.pkl')\n",
    "df_wd_no_lag = pd.read_pickle(featured_DATA_DIR + '/train_fin_wd_no_lag.pkl')\n",
    "df_wk_lag = pd.read_pickle(featured_DATA_DIR + '/train_fin_wk_lag.pkl')\n",
    "df_wk_no_lag = pd.read_pickle(featured_DATA_DIR + '/train_fin_wk_no_lag.pkl')\n",
    "#df_all_lag = pd.read_pickle(featured_DATA_DIR + '/train_fin_light_ver.pkl')\n",
    "#df_all = pd.read_pickle(featured_DATA_DIR + '/train_fin_wk_lag.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop unnecessary lag columns\n",
    "df_wd_lag = df_wd_lag.drop(columns = ['lag_sales_wk_1','lag_sales_wk_2'])\n",
    "df_wk_lag = df_wk_lag.drop(columns = ['lag_sales_wd_1', 'lag_sales_wd_2','lag_sales_wd_3', 'lag_sales_wd_4', 'lag_sales_wd_5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_col1 = ['lag_scode_count','lag_mcode_price','lag_mcode_count','lag_bigcat_price','lag_bigcat_count',\n",
    "            'lag_bigcat_price_day','lag_bigcat_count_day','lag_small_c_price','lag_small_c_count']\n",
    "\n",
    "lag_col2 = ['rolling_mean_7', 'rolling_mean_14', 'lag_sales_wd_1', 'lag_sales_wd_2','lag_sales_wd_3',\n",
    "            'lag_sales_wd_4', 'lag_sales_wd_5', 'lag_sales_wk_1','lag_sales_wk_2', 'ts_pred',\n",
    "           'rolling_mean_mcode_7','rolling_mean_mcode_14',]\n",
    "\n",
    "cat_col = ['상품군','weekdays','show_id','small_c','middle_c','big_c',\n",
    "                        'pay','months','hours_inweek','weekends','japp','parttime',\n",
    "                        'min_start','primetime','prime_origin','prime_smallc',\n",
    "                        'freq','bpower','steady','men','pay','luxury',\n",
    "                        'spring','summer','fall','winter','rain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple function that will be used for run_preprocess\n",
    "def na_to_zeroes(df):\n",
    "    \"\"\"\n",
    "    :objective: Change all na's to zero.(just for original lag!)\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    xcol = [x for x in df.columns if x in lag_col1+lag_col2]\n",
    "    for col in xcol:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_cat(df_pca):\n",
    "    \"\"\"\n",
    "    :objective: Before PCA, drop categorical variables\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    xcol = [x for x in df_pca.columns if x in cat_col+lag_col2]\n",
    "    df_pca = df_pca.drop(columns = xcol)\n",
    "    df_pca = df_pca.drop(columns = '취급액')\n",
    "\n",
    "    return df_pca\n",
    "\n",
    "def run_pca(df_pca_scaled, n_components = 5):\n",
    "    \"\"\"\n",
    "    :objective: Run PCA with n_components = 5\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components = 5)\n",
    "    pca.fit(df_pca_scaled)\n",
    "    df_pca = pca.transform(df_pca_scaled)\n",
    "\n",
    "    return df_pca\n",
    "\n",
    "## run preprocessing in a shot\n",
    "## pca is optional and only applied to numeric features other than 'lag'\n",
    "## NOTICE: removing outliers were run prior to dividing train/val\n",
    "## if replace = True, new PCA will replace corresponding numerical columns\n",
    "## if you want to simply add PCA columns to original data, set replace = False\n",
    "def run_preprocess(df, pca = True, replace = True):\n",
    "    \"\"\"\n",
    "    :objective: Run Feature deletion, NA imputation, label encoding, pca(optional)\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "#     df = drop_useless(df)\n",
    "    df = na_to_zeroes(df)\n",
    "    df = remove_outliers(df)\n",
    "    df = run_label_all(df)\n",
    "    df1 = df.copy()\n",
    "    if pca:\n",
    "        xcol = [x for x in df1.columns if x in cat_col+lag_col2]\n",
    "        df_pca = df1.copy()\n",
    "        df_pca = drop_cat(df_pca).copy()\n",
    "        df_pca = run_stdscale(df_pca)\n",
    "        df_pca = run_pca(df_pca)\n",
    "        if replace:\n",
    "            df_pca1 = pd.concat([df1[xcol], pd.DataFrame(df_pca)], axis=1)\n",
    "            return df_pca1\n",
    "        else:\n",
    "            df_pca2 = pd.concat([df1, pd.DataFrame(df_pca)], axis=1)\n",
    "            return df_pca2\n",
    "    else:\n",
    "        return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_wd_lag_PP = run_preprocess(df_wd_lag, pca = False, replace = False)\n",
    "# df_wd_no_lag_PP = run_preprocess(df_wd_no_lag, pca = True, replace =False)\n",
    "#df_wk_lag_PP = run_preprocess(df_wk_lag, pca = True, replace = False)\n",
    "#df_wk_no_lag_PP = run_preprocess(df_wk_no_lag, pca = True, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change here\n",
    "df_wd_lag_PP = na_to_zeroes(df_wd_lag)\n",
    "train1_x = df_wd_lag_PP.iloc[:16904,:].drop(columns = ['show_id','취급액'])\n",
    "train1_y = df_wd_lag_PP.iloc[:16904]['취급액']\n",
    "val1_x = df_wd_lag_PP.iloc[16904:,:].drop(columns = ['show_id','취급액'])\n",
    "val1_y = df_wd_lag_PP.iloc[16904:]['취급액']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    result = (-1)*mape\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN with categorical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  hours in week, small_c, middle_c, big_c, weekdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hours_inweek    100\n",
       "small_c         169\n",
       "middle_c         65\n",
       "big_c             9\n",
       "weekdays          5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1_x[['hours_inweek','small_c', 'middle_c', 'big_c', 'weekdays']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bayesian Methods for Hackers style sheet\n",
    "plt.style.use('bmh')\n",
    "\n",
    "np.random.seed(1234567890)\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicLogger(Callback):\n",
    "    \"\"\"\n",
    "    A helper callback class that only prints the losses once in 'display' epochs\n",
    "    \"\"\"\n",
    "    def __init__(self, display=100):\n",
    "        self.display = display\n",
    "\n",
    "    def on_train_begin(self, logs={}):      \n",
    "        self.epochs = 0    \n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        self.epochs += 1     \n",
    "        if self.epochs % self.display == 0:\n",
    "            print (\"Epoch: %d - loss: %f - val_loss: %f\" % (self.epochs, logs['loss'], logs['val_loss']))\n",
    " \n",
    "            \n",
    "periodic_logger_250 = PeriodicLogger(250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the continuous and categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change here\n",
    "\n",
    "# continuous variables\n",
    "continuous_cols = ['rolling_mean_7', 'rolling_mean_14', 'rolling_mean_mcode_7',\n",
    "       'rolling_mean_mcode_14', 'lag_sales_wd_1', 'lag_sales_wd_2',\n",
    "       'lag_sales_wd_3', 'lag_sales_wd_4', 'lag_sales_wd_5', 'ts_pred']\n",
    "# categorical variables\n",
    "categorical_cols = ['hours_inweek','small_c', 'middle_c', 'big_c', 'weekdays']\n",
    "\n",
    "X_train_continuous = train1_x[continuous_cols]\n",
    "X_train_categorical = train1_x[categorical_cols]\n",
    "y_train = train1_y\n",
    "\n",
    "X_val_continuous = val1_x[continuous_cols]\n",
    "X_val_categorical = val1_x[categorical_cols]\n",
    "y_val = val1_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16904     5600000.0\n",
       "16905    10638000.0\n",
       "16906    11241000.0\n",
       "16907    25385000.0\n",
       "16908    24011000.0\n",
       "16909    42821000.0\n",
       "16910    14583000.0\n",
       "16911    26732000.0\n",
       "16912    26254000.0\n",
       "16913    25153000.0\n",
       "Name: 취급액, dtype: float64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing both train and test sets to have 0 mean and std. of 1 using the train set mean and std.\n",
    "# This will give each feature an equal initial importance and speed up the training time\n",
    "train_mean = X_train_continuous.mean(axis=0)\n",
    "train_std = X_train_continuous.std(axis=0)\n",
    "\n",
    "X_train_continuous = X_train_continuous - train_mean\n",
    "X_train_continuous /= train_std\n",
    "\n",
    "X_val_continuous = X_val_continuous - train_mean\n",
    "X_val_continuous /= train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model using a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingMapping():\n",
    "    \"\"\"\n",
    "    Helper class for handling categorical variables\n",
    "    \n",
    "    An instance of this class should be defined for each categorical variable we want to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, series):\n",
    "        # get a list of unique values\n",
    "        values = series.unique().tolist()\n",
    "        \n",
    "        # Set a dictionary mapping from values to integer value\n",
    "        self.embedding_dict = {value: int_value+1 for int_value, value in enumerate(values)}\n",
    "        \n",
    "        # The num_values will be used as the input_dim when defining the embedding layer. \n",
    "        # It will also be returned for unseen values \n",
    "        self.num_values = len(values) + 1\n",
    "\n",
    "    def get_mapping(self, value):\n",
    "        # If the value was seen in the training set, return its integer mapping\n",
    "        if value in self.embedding_dict:\n",
    "            return self.embedding_dict[value]\n",
    "        \n",
    "        # Else, return the same integer for unseen values\n",
    "        else:\n",
    "            return self.num_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an embedding column for the train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change here\n",
    "# if you change categorical variables used in the model,\n",
    "# you have to create an embedding column \n",
    "hours_inweek_mapping = EmbeddingMapping(X_train_categorical['hours_inweek'])\n",
    "small_c_mapping = EmbeddingMapping(X_train_categorical['small_c'])\n",
    "middle_c_mapping = EmbeddingMapping(X_train_categorical['middle_c'])\n",
    "big_c_mapping = EmbeddingMapping(X_train_categorical['big_c'])\n",
    "weekdays_mapping = EmbeddingMapping(X_train_categorical['weekdays'])\n",
    "\n",
    "\n",
    "X_train_categorical = X_train_categorical.assign(\n",
    "    hours_inweek_mapping=X_train_categorical['hours_inweek'].apply(hours_inweek_mapping.get_mapping),\n",
    "    small_c_mapping=X_train_categorical['small_c'].apply(small_c_mapping.get_mapping),\n",
    "    middle_c_mapping=X_train_categorical['middle_c'].apply(middle_c_mapping.get_mapping),\n",
    "    big_c_mapping=X_train_categorical['big_c'].apply(big_c_mapping.get_mapping),\n",
    "    weekdays_mapping=X_train_categorical['weekdays'].apply(weekdays_mapping.get_mapping))\n",
    "\n",
    "X_val_categorical = X_val_categorical.assign(\n",
    "    hours_inweek_mapping=X_val_categorical['hours_inweek'].apply(hours_inweek_mapping.get_mapping),\n",
    "    small_c_mapping=X_val_categorical['small_c'].apply(small_c_mapping.get_mapping),\n",
    "    middle_c_mapping=X_val_categorical['middle_c'].apply(middle_c_mapping.get_mapping),\n",
    "    big_c_mapping=X_val_categorical['big_c'].apply(big_c_mapping.get_mapping),\n",
    "    weekdays_mapping=X_val_categorical['weekdays'].apply(weekdays_mapping.get_mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dense_features = len(continuous_cols)\n",
    "# change here\n",
    "lr = 0.002\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# Dense input\n",
    "# for continuous variables\n",
    "dense_input = Input(shape=(num_dense_features, ), name='dense1')\n",
    "\n",
    "# Embedding input\n",
    "# change here if you add another categorical variable\n",
    "hours_inweek_input = Input(shape=(1,), name='hours_inweek')\n",
    "small_c_input = Input(shape=(1,), name='small_c')\n",
    "middle_c_input = Input(shape=(1,), name='middle_c')\n",
    "big_c_input = Input(shape=(1,), name='big_c')\n",
    "weekdays_input = Input(shape=(1,), name='weekdays')\n",
    "\n",
    "# if cardinality of a categorical variable is over 50,\n",
    "# Howard suggests to set the output dim to be 50 \n",
    "# to avoid problems from high cardinality\n",
    "hours_inweek_emb = Flatten()(Embedding(input_dim=hours_inweek_mapping.num_values+1, output_dim = 50)(hours_inweek_input))\n",
    "small_c_emb = Flatten()(Embedding(input_dim=small_c_mapping.num_values+1, output_dim = 50)(small_c_input))\n",
    "middle_c_emb = Flatten()(Embedding(input_dim=middle_c_mapping.num_values+1, output_dim = 50)(middle_c_input))\n",
    "big_c_emb = Flatten()(Embedding(input_dim=big_c_mapping.num_values+1, output_dim = 9)(big_c_input))\n",
    "weekdays_emb = Flatten()(Embedding(input_dim=weekdays_mapping.num_values+1, output_dim = 5)(weekdays_input))\n",
    "\n",
    "# Combine dense and embedding parts and add dense layers. Exit on linear scale.\n",
    "# change here if you add a categorical var\n",
    "x = concatenate([dense_input, \n",
    "                 hours_inweek_emb, \n",
    "                 small_c_emb, \n",
    "                 middle_c_emb, \n",
    "                 big_c_emb, \n",
    "                 weekdays_emb])\n",
    "\n",
    "# change here if needed\n",
    "x = Dense(256*2, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256*2, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(128*2, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128*2, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64*2, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(16*2, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(4*2, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "outputs = Dense(1, activation=\"linear\", name='output')(x)\n",
    "\n",
    "# change here if you add sth\n",
    "inputs = {\"dense1\": dense_input,\n",
    "          \"hours_inweek\": hours_inweek_input, \n",
    "          \"small_c\": small_c_input, \n",
    "          \"middle_c\": middle_c_input,\n",
    "          \"big_c\": big_c_input, \n",
    "          \"weekdays\": weekdays_input}\n",
    "\n",
    "# Connect input and output\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=keras.losses.mean_absolute_percentage_error,\n",
    "              metrics=[\"mape\"], \n",
    "              optimizer=keras.optimizers.Adam(learning_rate=lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "hours_inweek (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "small_c (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "middle_c (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "big_c (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "weekdays (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 50)        5150        hours_inweek[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 50)        8600        small_c[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 50)        3400        middle_c[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 9)         108         big_c[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 5)         40          weekdays[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (InputLayer)             [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 50)           0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 50)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 50)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 9)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 5)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 174)          0           dense1[0][0]                     \n",
      "                                                                 flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          89600       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512)          2048        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          65792       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256)          1024        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          32896       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128)          512         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           4128        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32)           128         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 8)            264         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8)            32          dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            9           batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 610,787\n",
      "Trainable params: 607,379\n",
      "Non-trainable params: 3,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change here depending on categorical variables used\n",
    "train_inputs = {\"dense1\": X_train_continuous,\n",
    "          \"hours_inweek\": X_train_categorical['hours_inweek_mapping'], \n",
    "          \"small_c\": X_train_categorical['small_c_mapping'], \n",
    "          \"middle_c\": X_train_categorical['middle_c_mapping'],\n",
    "          \"big_c\": X_train_categorical['big_c_mapping'], \n",
    "          \"weekdays\": X_train_categorical['weekdays_mapping']}\n",
    "\n",
    "val_inputs = {\"dense1\": X_val_continuous,\n",
    "          \"hours_inweek\": X_val_categorical['hours_inweek_mapping'], \n",
    "          \"small_c\": X_val_categorical['small_c_mapping'], \n",
    "          \"middle_c\": X_val_categorical['middle_c_mapping'],\n",
    "          \"big_c\": X_val_categorical['big_c_mapping'], \n",
    "          \"weekdays\": X_val_categorical['weekdays_mapping']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change here\n",
    "epochs = 10000\n",
    "# Note continuous and categorical columns are inserted in the same order as defined in all_inputs\n",
    "history = model.fit(train_inputs, \n",
    "                    y_train, epochs=epochs, batch_size=128, \n",
    "          callbacks=[periodic_logger_250], verbose=0,\n",
    "          validation_data=(val_inputs, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train/validation loss values\n",
    "plt.figure(figsize=(20,10))\n",
    "_loss = history.history['loss'][250:]\n",
    "_val_loss = history.history['val_loss'][250:]\n",
    "\n",
    "train_loss_plot, = plt.plot(range(1, len(_loss)+1), _loss, label='Train Loss')\n",
    "val_loss_plot, = plt.plot(range(1, len(_val_loss)+1), _val_loss, label='Validation Loss')\n",
    "\n",
    "_ = plt.legend(handles=[train_loss_plot, val_loss_plot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"This is the average value we are trying to predict: %d\" % y_val.mean().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good are the model's predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = y_val.copy()\n",
    "\n",
    "# Add a column for the model's predicted values\n",
    "df['pred'] = model.predict(val_inputs)\n",
    "\n",
    "# Calculate the difference between the predicted and the actual price\n",
    "df['diff'] = df['pred'] - df['취급액']\n",
    "\n",
    "# Calculate the absolute difference between the predicted and the actual price\n",
    "df['abs_diff'] = np.abs(df['diff'])\n",
    "\n",
    "# Calculate the percentage of the difference from the actual price\n",
    "df['%diff'] = 100 * (df['diff'] / df['취급액'])\n",
    "\n",
    "# Calculate the absolute percentage difference from the actual price\n",
    "df['abs_%diff'] = np.abs(df['%diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the biggest difference in absolute values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the 'abs_diff' field and show the 5 largest mistakes in absolute values\n",
    "df.sort_values(\"abs_diff\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and std. of the diff field\n",
    "diff_mean, diff_std = df['diff'].mean(), df['diff'].std()\n",
    "print(\"The mean is very close to 0 ({mean}) with std. {std}.\".format(mean=round(diff_mean, 2), std=round(diff_std, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the histogram of the differences\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(df['diff'], bins=100)\n",
    "plt.xlabel(\"$\")\n",
    "plt.ylabel(\"# samples\")\n",
    "_ = plt.title(\"Difference between predicted and actual price\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the biggest difference in percentage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the '%diff' field and show the 5 largest proportional mistakes\n",
    "df.sort_values(\"abs_%diff\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, plot the histogram\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(df['%diff'], bins=100)\n",
    "plt.xlabel(\"%\")\n",
    "plt.ylabel(\"# samples\")\n",
    "_ = plt.title(\"% of difference between predicted and actual price\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
