{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# practice run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "# data\n",
    "import datetime\n",
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# sklearn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# custom class\n",
    "from features_yj import Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Features()\n",
    "df = t.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "# data\n",
    "import datetime\n",
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# sklearn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# custom class\n",
    "from features_yj import Features\n",
    "\n",
    "\n",
    "# if separate df with all features is existent, set path\n",
    "# otherwise merge raw df with Features module\n",
    "\n",
    "df_path = 'path of df'\n",
    "\n",
    "def load_df_added(df, path = True):\n",
    "    \"\"\"\n",
    "    :objective: load data\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    if path :\n",
    "        df = pd.read_pickle(df_path)\n",
    "    else:\n",
    "        t = Features()\n",
    "        df = t.run_all()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# set 'keepshowid = False' if you don't want it\n",
    "def drop_useless(df, keepshowid = True):\n",
    "    \"\"\"\n",
    "    :objective: drop useless features for model. save 'show_id' just in case\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    #useless features\n",
    "    xcol = ['방송일시', '노출(분)', '마더코드', '상품코드', '상품명', 'exposed', 'ymd', 'volume',\n",
    "            'years','days','hours','week_num','holidays', 'red', 'min_range','brand','original_c',\n",
    "            'small_c_code','middle_c_code','big_c_code','sales_power']\n",
    "    col = [x for x in df.columns if x in xcol]\n",
    "    df = df.drop(columns = col)\n",
    "    if keepshowid:\n",
    "        df = df.copy()\n",
    "    else:\n",
    "        df = df.drop(columns = ['show_id'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def check_na(df):\n",
    "    \"\"\"\n",
    "    :objective: show na\n",
    "    :return: columns with na / na counts\n",
    "    \"\"\"\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "# 새로운 변수에 na 있으면 일단 imputation 따로 해야함. 아니면 여기 list에 변수 이름 추가해도 됨.\n",
    "def na_to_zeroes(df):\n",
    "    \"\"\"\n",
    "    :objective: Change all na's to zero.(just for original lag!)\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    lag_col = ['lag_scode_count','lag_mcode_price','lag_mcode_count','lag_bigcat_price','lag_bigcat_count',\n",
    "                'lag_bigcat_price_day','lag_bigcat_count_day','lag_small_c_price','lag_small_c_count']\n",
    "    for col in lag_col:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "## Encoding\n",
    "# One-hot-Encoding\n",
    "def run_onehot(df):\n",
    "    \"\"\"\n",
    "    :objective: Perform ohe for categorical columns\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    cat_col = ['min_start','japp','parttime', 'primetime','exposed_t','상품군','weekdays', 'small_c','middle_c','big_c', 'pay','men']\n",
    "    num = df.drop(columns = cat_col)\n",
    "    X1 = df[cat_col]\n",
    "    # Onehotencoder\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(X1)\n",
    "    onehotlabels = ohe.transform(X1).toarray()\n",
    "    cat_labels = ohe.get_feature_names(['min_start','japp','parttime', 'primetime','exposed','상품군','weekdays', 'small_c','middle_c','big_c', 'pay','men'])\n",
    "    cat = pd.DataFrame(onehotlabels, columns=cat_labels)\n",
    "    df_ohe = num.join(cat)\n",
    "\n",
    "    return df_ohe\n",
    "\n",
    "# Label Encoding\n",
    "def get_label_features(df):\n",
    "    \"\"\"\n",
    "    :objective: Show features that need labelencoding\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    lab_col = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "    return lab_col\n",
    "\n",
    "\n",
    "def run_label_separately(df, colname):\n",
    "    \"\"\"\n",
    "    :objective: Perform labelencoding for selected column(only one)\n",
    "    :return: pandas dataframe with encoding only on selected column\n",
    "    \"\"\"\n",
    "    if type(colname) == str:\n",
    "        le = LabelEncoder()\n",
    "        df[colname] = le.fit_transform(df[colname].values)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Error! [colname] should be string type.\")\n",
    "\n",
    "\n",
    "def get_label_mapping(le):\n",
    "    \"\"\"\n",
    "    :objective: Return a dict mapping labels to their integer values / Run right after 'run_label_separately' / le = a fitted SKlearn LabelEncoder\n",
    "    :return: integer\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for cl in le.classes_:\n",
    "        res.update({cl:le.transform([cl])[0]})\n",
    "\n",
    "    return res\n",
    "\n",
    "#integerMapping = get_label_mapping(le)\n",
    "#integerMapping['Monday'] #하나만 보고 싶을 때\n",
    "#이미 한번 인코딩을 한 변수에 또 labelencoding을 할 경우 mapping 못불러옴\n",
    "#get_label_features 아웃풋 보고 한번씩 separate하게 인코딩 하고, 인코딩 한번 할 때마다 get_label_mapping(le)으로 확인하는거 추천\n",
    "#get_label_features 확인 -> run_label_separately(df, colname) -> get_label_mapping(le) 저장 -> run_label_separately(df, colname) -> get_label_mapping(le) 저장\n",
    "\n",
    "\n",
    "# mapping 안궁금하고 그냥 한꺼번에 다 돌리기\n",
    "def run_label_all(df):\n",
    "    \"\"\"\n",
    "    :objective: Perform labelencoding for all categorical/object columns\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    lab_col = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "    for col in lab_col:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].values)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# remove outliers\n",
    "def remove_outliers(df_train):\n",
    "    \"\"\"\n",
    "    :objective: Remove outliers. Before dividing into X/y\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    numeric_colnum = df_train.columns.get_indexer(['판매단가','취급액','vratings']).tolist()\n",
    "    feature_set = df_train.iloc[:,numeric_colnum]\n",
    "    # identify outliers in the training dataset\n",
    "    iso = IsolationForest(n_estimators=50, max_samples=50, contamination=float(0.05),max_features=1.0)\n",
    "    iso.fit(feature_set)\n",
    "    pred = iso.predict(feature_set)\n",
    "    feature_set['anomaly']=pred\n",
    "    outliers=feature_set.loc[feature_set['anomaly']==-1]\n",
    "    outlier_index=list(outliers.index)\n",
    "    df_train = df_train.loc[~df_train.index.isin(outlier_index)].reset_index()\n",
    "\n",
    "    return df_train\n",
    "\n",
    "\n",
    "## PCA\n",
    "# global vars\n",
    "categorical_features = ['상품군','weekdays','show_id','small_c','middle_c','big_c',\n",
    "                        'pay','months','hours_inweek','weekends','japp','parttime',\n",
    "                        'min_start','primetime','prime_origin','prime_smallc',\n",
    "                        'freq','bpower','steady','men','pay','luxury',\n",
    "                        'spring','summer','fall','winter','rain']\n",
    "\n",
    "def drop_cat(df_pca):\n",
    "    \"\"\"\n",
    "    :objective: Before PCA, drop categorical variables\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    df_pca = df_pca.drop(columns = categorical_features)\n",
    "\n",
    "    return df_pca\n",
    "\n",
    "# scalers\n",
    "# Min-max가 좀 더 일반적이지만 Standard는 outlier 영향을 적게 받는다는 장점이 있습니당\n",
    "\n",
    "def run_stdscale(df_pca):\n",
    "    \"\"\"\n",
    "    :objective: scale / all columns should be numeric!!!\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(df_pca)\n",
    "\n",
    "    return scaled\n",
    "\n",
    "def run_pca(df_pca_scaled, n_components = 10):\n",
    "    \"\"\"\n",
    "    :objective: Run PCA with n_components = 10\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=10)\n",
    "    pca.fit(df_pca_scaled)\n",
    "    df_pca = pca.transform(df_pca_scaled)\n",
    "\n",
    "    return df_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2 = drop_useless(df2)\n",
    "df2 = na_to_zeroes(df2)\n",
    "df2 = run_label_all(df2)\n",
    "df_pca = drop_cat(df2)\n",
    "df_pca = run_stdscale(df_pca)\n",
    "df_pca = run_pca(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_and_pca = pd.concat([df2[categorical_features], df_pca], axis=1)\n",
    "df_all_and_pca = pd.concat([df2, df_pca], axis=1)\n",
    "df_all = df2\n",
    "\n",
    "df1_train = df_cat_and_pca.iloc[:23813,:]\n",
    "df1_test = df_cat_and_pca.iloc[23813:,:]\n",
    "\n",
    "df2_train = df_all_and_pca.iloc[:23813,:]\n",
    "df2_test = df_all_and_pca.iloc[23813:,:]\n",
    "\n",
    "df3_train = df_all.iloc[:23813,:]\n",
    "df3_test = df_all.iloc[23813:,:]\n",
    "\n",
    "df1_train.to_csv ('cat_and_pca_train.csv', index = False, header=True,encoding='ms949')\n",
    "df2_train.to_csv ('all_and_pca_train.csv', index = False, header=True,encoding='ms949')\n",
    "df3_train.to_csv ('all_train.csv', index = False, header=True,encoding='ms949')\n",
    "\n",
    "df1_test.to_csv ('cat_and_pca_test.csv', index = False, header=True,encoding='ms949')\n",
    "df2_test.to_csv ('all_and_pca_test.csv', index = False, header=True,encoding='ms949')\n",
    "df3_test.to_csv ('all_test.csv', index = False, header=True,encoding='ms949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23813, 37)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = df1_train\n",
    "#X_test = df1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df1_train.drop(columns = ['취급액'])\n",
    "X_test = df1_test.drop(columns = ['취급액'])\n",
    "y_train = df1_train['취급액']\n",
    "y_test = df1_test['취급액']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
